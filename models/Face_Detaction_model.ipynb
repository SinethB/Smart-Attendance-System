{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cIFnrRMduAMq"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import cv2\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MFOdLTNLOXim"
      },
      "outputs": [],
      "source": [
        "# Step 1: Unzip the Dataset\n",
        "zip_path = '/content/drive/MyDrive/computer vision/Face detection.v1-fddb.tensorflow.zip'\n",
        "unzip_path = '/content/Face detection'\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(unzip_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install openvino-dev\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xVqF_s-HTiG",
        "outputId": "466e455c-bc6c-4a6f-bd2d-a88fd16c296e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openvino-dev in /usr/local/lib/python3.10/dist-packages (2024.6.0)\n",
            "Requirement already satisfied: defusedxml>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from openvino-dev) (0.7.1)\n",
            "Requirement already satisfied: networkx<=3.1.0 in /usr/local/lib/python3.10/dist-packages (from openvino-dev) (3.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.16.6 in /usr/local/lib/python3.10/dist-packages (from openvino-dev) (1.26.4)\n",
            "Requirement already satisfied: openvino-telemetry>=2023.2.1 in /usr/local/lib/python3.10/dist-packages (from openvino-dev) (2024.5.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from openvino-dev) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from openvino-dev) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.25.1 in /usr/local/lib/python3.10/dist-packages (from openvino-dev) (2.32.3)\n",
            "Requirement already satisfied: openvino==2024.6.0 in /usr/local/lib/python3.10/dist-packages (from openvino-dev) (2024.6.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.25.1->openvino-dev) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.25.1->openvino-dev) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.25.1->openvino-dev) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.25.1->openvino-dev) (2024.12.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTdrK2FOHNxP",
        "outputId": "533310a9-3f1d-4d9f-c0de-295099e8fd03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
            "\u001b[1m1305/1305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m318s\u001b[0m 200ms/step - loss: 0.7952 - val_loss: 0.8653\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model training completed and saved at '/content/face_detection_model.h5'.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Step 2: Load and Prepare Data\n",
        "def load_annotations(folder_path):\n",
        "    annotations_file = os.path.join(folder_path, '_annotations.csv')\n",
        "    annotations = pd.read_csv(annotations_file)\n",
        "    data = []\n",
        "    for _, row in annotations.iterrows():\n",
        "        if row['class'] == 'face':\n",
        "            image_path = os.path.join(folder_path, row['filename'])\n",
        "            if os.path.exists(image_path):\n",
        "                data.append({\n",
        "                    'filename': image_path,\n",
        "                    'bbox': [row['xmin'], row['ymin'], row['xmax'], row['ymax']],\n",
        "                })\n",
        "            else:\n",
        "                print(f\"Warning: Missing file - {image_path}\")\n",
        "    return data\n",
        "\n",
        "train_data = load_annotations(os.path.join(unzip_path, 'train'))\n",
        "valid_data = load_annotations(os.path.join(unzip_path, 'valid'))\n",
        "\n",
        "# Step 3: Data Generator\n",
        "def preprocess_image(image_path, bbox, target_size=(224, 224)):\n",
        "    img = cv2.imread(image_path)\n",
        "    if img is None:\n",
        "        print(f\"Error: Unable to read image {image_path}\")\n",
        "        return None, None\n",
        "    img = cv2.resize(img, target_size)\n",
        "    bbox = [coord / max(img.shape[:2]) for coord in bbox]  # Normalize bounding box\n",
        "    return img, bbox\n",
        "\n",
        "def data_generator(data, batch_size=32, target_size=(224, 224)):\n",
        "    while True:\n",
        "        batch_images, batch_bboxes = [], []\n",
        "        for _ in range(batch_size):\n",
        "            idx = np.random.randint(len(data))\n",
        "            item = data[idx]\n",
        "            img, bbox = preprocess_image(item['filename'], item['bbox'], target_size)\n",
        "            if img is not None and bbox is not None:\n",
        "                batch_images.append(img)\n",
        "                batch_bboxes.append(bbox)\n",
        "        yield np.array(batch_images) / 255.0, np.array(batch_bboxes)\n",
        "\n",
        "# Step 4: Build Model\n",
        "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "x = Flatten()(base_model.output)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "outputs = Dense(4, activation='sigmoid')(x)\n",
        "model = Model(inputs=base_model.input, outputs=outputs)\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=1e-4), loss='mean_squared_error')\n",
        "\n",
        "# Step 5: Train Model\n",
        "train_gen = data_generator(train_data)\n",
        "valid_gen = data_generator(valid_data)\n",
        "\n",
        "steps_per_epoch = len(train_data) // 32\n",
        "validation_steps = len(valid_data) // 32\n",
        "\n",
        "model.fit(\n",
        "    train_gen,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    validation_data=valid_gen,\n",
        "    validation_steps=validation_steps,\n",
        "    epochs=1,\n",
        ")\n",
        "\n",
        "# Save the model\n",
        "model.save('/content/face_detection_model.h5')\n",
        "\n",
        "print(\"Model training completed and saved at '/content/face_detection_model.h5'.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FEAGH7-3NPhs",
        "outputId": "05502eba-24e5-4266-854d-8bd0b40a3dbb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ],
      "source": [
        "# Save the model\n",
        "model.save('/content/drive/MyDrive/computer vision/face_detection_model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mo --input_model /content/face_detection_model.h5 --framework tf --output_dir /content/output\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vaNgBRMWHZAm",
        "outputId": "1ba3e3a2-a760-485a-a2c9-e34f496c9f19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ INFO ] MO command line tool is considered as the legacy conversion API as of OpenVINO 2023.2 release.\n",
            "In 2025.0 MO command line tool and openvino.tools.mo.convert_model() will be removed. Please use OpenVINO Model Converter (OVC) or openvino.convert_model(). OVC represents a lightweight alternative of MO and provides simplified model conversion API. \n",
            "Find more information about transition from MO to OVC at https://docs.openvino.ai/2023.2/openvino_docs_OV_Converter_UG_prepare_model_convert_model_MO_OVC_transition.html\n",
            "2025-01-10 03:31:09.983478: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-01-10 03:31:10.004613: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-01-10 03:31:10.010805: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-01-10 03:31:11.188360: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[libprotobuf ERROR ../../../../../repos/openvino/thirdparty/protobuf/protobuf/src/google/protobuf/text_format.cc:337] Error parsing text-format tensorflow.GraphDef: 1:1: Interpreting non ascii codepoint 137.\n",
            "[libprotobuf ERROR ../../../../../repos/openvino/thirdparty/protobuf/protobuf/src/google/protobuf/text_format.cc:337] Error parsing text-format tensorflow.GraphDef: 1:1: Expected identifier, got: �\n",
            "[ ERROR ]  -------------------------------------------------\n",
            "[ ERROR ]  ----------------- INTERNAL ERROR ----------------\n",
            "[ ERROR ]  Unexpected exception happened.\n",
            "[ ERROR ]  Please contact Model Optimizer developers and forward the following information:\n",
            "[ ERROR ]  Check 'false' failed at src/frontends/tensorflow/src/frontend.cpp:409:\n",
            "FrontEnd API failed with GeneralFailure:\n",
            "[TensorFlow Frontend] Internal error or inconsistent input model: the frontend supports frozen formats (.pb and .pbtxt), SavedModel and MetaGraph (.meta), and v1 checkpoints.\n",
            "\n",
            "[ ERROR ]  Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openvino/tools/mo/convert_impl.py\", line 882, in _convert\n",
            "    ov_model, legacy_path = driver(argv, {\"conversion_parameters\": non_default_params})\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openvino/tools/mo/convert_impl.py\", line 542, in driver\n",
            "    graph, ngraph_function = prepare_ir(argv)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openvino/tools/mo/convert_impl.py\", line 396, in prepare_ir\n",
            "    ngraph_function = moc_pipeline(argv, moc_front_end)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openvino/tools/mo/moc_frontend/pipeline.py\", line 56, in moc_pipeline\n",
            "    input_model = moc_front_end.load(argv.input_model, share_weights)\n",
            "openvino._pyopenvino.GeneralFailure: Check 'false' failed at src/frontends/tensorflow/src/frontend.cpp:409:\n",
            "FrontEnd API failed with GeneralFailure:\n",
            "[TensorFlow Frontend] Internal error or inconsistent input model: the frontend supports frozen formats (.pb and .pbtxt), SavedModel and MetaGraph (.meta), and v1 checkpoints.\n",
            "\n",
            "\n",
            "[ ERROR ]  ---------------- END OF BUG REPORT --------------\n",
            "[ ERROR ]  -------------------------------------------------\n",
            "[ INFO ] You can also try to use legacy TensorFlow Frontend by using argument --use_legacy_frontend.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Unzip the Dataset\n",
        "zip_path = '/content/drive/MyDrive/computer vision/Face detection.v1-fddb.tensorflow.zip'\n",
        "unzip_path = '/content/Face detection'\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(unzip_path)\n",
        "\n",
        "# Step 2: Load and Prepare Data\n",
        "def load_annotations(folder_path):\n",
        "    annotations_file = os.path.join(folder_path, '_annotations.csv')\n",
        "    annotations = pd.read_csv(annotations_file)\n",
        "    data = []\n",
        "    for _, row in annotations.iterrows():\n",
        "        if row['class'] == 'face':\n",
        "            image_path = os.path.join(folder_path, row['filename'])\n",
        "            if os.path.exists(image_path):\n",
        "                data.append({\n",
        "                    'filename': image_path,\n",
        "                    'bbox': [row['xmin'], row['ymin'], row['xmax'], row['ymax']],\n",
        "                })\n",
        "            else:\n",
        "                print(f\"Warning: Missing file - {image_path}\")\n",
        "    return data\n",
        "\n",
        "train_data = load_annotations(os.path.join(unzip_path, 'train'))\n",
        "valid_data = load_annotations(os.path.join(unzip_path, 'valid'))\n",
        "\n",
        "# Step 3: Data Generator\n",
        "def preprocess_image(image_path, bbox, target_size=(224, 224)):\n",
        "    img = cv2.imread(image_path)\n",
        "    if img is None:\n",
        "        print(f\"Error: Unable to read image {image_path}\")\n",
        "        return None, None\n",
        "    img = cv2.resize(img, target_size)\n",
        "    bbox = [coord / max(img.shape[:2]) for coord in bbox]  # Normalize bounding box\n",
        "    return img, bbox\n",
        "\n",
        "def data_generator(data, batch_size=32, target_size=(224, 224)):\n",
        "    while True:\n",
        "        batch_images, batch_bboxes = [], []\n",
        "        for _ in range(batch_size):\n",
        "            idx = np.random.randint(len(data))\n",
        "            item = data[idx]\n",
        "            img, bbox = preprocess_image(item['filename'], item['bbox'], target_size)\n",
        "            if img is not None and bbox is not None:\n",
        "                batch_images.append(img)\n",
        "                batch_bboxes.append(bbox)\n",
        "        yield np.array(batch_images) / 255.0, np.array(batch_bboxes)\n",
        "\n",
        "# Step 4: Build Model\n",
        "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "x = Flatten()(base_model.output)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "outputs = Dense(4, activation='sigmoid')(x)\n",
        "model = Model(inputs=base_model.input, outputs=outputs)\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=1e-4), loss='mean_squared_error')\n",
        "\n",
        "# Step 5: Train Model\n",
        "train_gen = data_generator(train_data)\n",
        "valid_gen = data_generator(valid_data)\n",
        "\n",
        "steps_per_epoch = len(train_data) // 32\n",
        "validation_steps = len(valid_data) // 32\n",
        "\n",
        "model.fit(\n",
        "    train_gen,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    validation_data=valid_gen,\n",
        "    validation_steps=validation_steps,\n",
        "    epochs=1,\n",
        ")\n",
        "\n",
        "# Save the model as a .pb file\n",
        "tf.saved_model.save(model, \"/content/saved_model\")\n",
        "\n",
        "# Convert to OpenVINO format\n",
        "print(\"Converting the model to OpenVINO format...\")\n",
        "os.system(f\"\"\"\n",
        "    mo --saved_model_dir /content/saved_model \\\n",
        "       --output_dir /content/openvino_model \\\n",
        "       --input_shape \"[1,224,224,3]\" \\\n",
        "       --data_type FP16\n",
        "\"\"\")\n",
        "print(\"Model converted and saved as .xml and .bin in '/content/openvino_model'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrgarV5LBviP",
        "outputId": "200f54f9-f8fb-4b5e-86c1-dc4217e05a3b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "\u001b[1m1305/1305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m291s\u001b[0m 185ms/step - loss: 0.7787 - val_loss: 0.8438\n",
            "Converting the model to OpenVINO format...\n",
            "Model converted and saved as .xml and .bin in '/content/openvino_model'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Converting the model to OpenVINO format...\")\n",
        "os.system(f\"\"\"\n",
        "    mo --saved_model_dir /content/saved_model \\\n",
        "       --output_dir /content/xmll \\\n",
        "       --input_shape \"[1,224,224,3]\" \\\n",
        "       --data_type FP16\n",
        "\"\"\")\n",
        "print(\"Model converted and saved as .xml and .bin in '/content/xmll'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVcUKgxnDqSU",
        "outputId": "5cf97eb8-0b3d-46be-fed9-6d063515b379"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converting the model to OpenVINO format...\n",
            "Model converted and saved as .xml and .bin in '/content/xmll'.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}